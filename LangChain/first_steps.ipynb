{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f315b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ea7704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88e0ffb",
   "metadata": {},
   "source": [
    "# First steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "813e0ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79713abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Large language models work by using a vast amount of text data to learn patterns and relationships between words. These models are typically trained on massive datasets of text, such as books, articles, and websites, in order to understand the structure and meaning of language.\\n\\nThe models use a technique called deep learning, which involves passing the text data through multiple layers of interconnected neurons to extract and learn the underlying patterns. These models are trained to predict the next word in a sentence based on the context of the previous words, allowing them to generate coherent and contextually relevant text.\\n\\nLarge language models can be used for a variety of natural language processing tasks, such as text generation, translation, summarization, and sentiment analysis. They have become increasingly popular in recent years due to their ability to generate human-like text and perform complex language tasks with high accuracy.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LLM invoke\n",
    "llm.invoke(\"Can you explain how large language models work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06587baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure! Large language models, such as GPT-3 (Generative Pre-trained Transformer 3), are based on deep learning techniques called transformers. These models are trained on massive amounts of text data to understand and generate human-like text.\\n\\nHere's a simplified explanation of how large language models work:\\n\\n1. **Input Encoding**: The model takes an input sequence of text tokens (words or subwords) and converts them into numerical representations using embeddings.\\n\\n2. **Transformer Architecture**: Large language models use a transformer architecture, which consists of multiple layers of self-attention mechanisms and feedforward neural networks. Self-attention allows the model to weigh the importance of different words in the input sequence when generating the output.\\n\\n3. **Training**: The model is trained on a large dataset using unsupervised learning techniques. During training, the model learns to predict the next word in a sequence given the previous words. This process helps the model learn the underlying patterns and relationships in the text data.\\n\\n4. **Fine-Tuning**: After pre-training on a large dataset, the model can be fine-tuned on specific tasks or datasets to improve its performance on particular tasks, such as text generation, question-answering, or language translation.\\n\\n5. **Generation**: Once trained, the model can generate text by predicting the next word in a sequence based on the input text and the learned patterns from the training data. The generated text is often coherent and contextually relevant, making it appear human-like.\\n\\nOverall, large language models leverage the power of deep learning and massive amounts of data to understand and generate natural language text. They have been used in a wide range of applications, from chatbots and virtual assistants to language translation and content generation.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "#Prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a university teacher assistant.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "#LLM chain\n",
    "chain = prompt | llm \n",
    "\n",
    "chain.invoke({\"input\": \"Can you explain how large language models work?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dddd8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Certainly! Large language models, such as GPT-3 (Generative Pre-trained Transformer 3), are based on a type of artificial intelligence called deep learning. These models are designed to understand and generate human language text.\\n\\nHere's a simplified explanation of how large language models like GPT-3 work:\\n\\n1. **Training Data**: Large language models are trained on massive amounts of text data from the internet. This data includes books, articles, websites, and other sources of written language. The model learns the patterns and structures of language from this training data.\\n\\n2. **Transformer Architecture**: Large language models like GPT-3 are built on a transformer architecture, which allows them to process and generate text by considering the context of words and sentences. Transformers use attention mechanisms to focus on different parts of the input text when generating output.\\n\\n3. **Pre-training and Fine-tuning**: Before being used for specific tasks, large language models are pre-trained on a general language understanding task. This pre-training phase helps the model learn the nuances of language. After pre-training, the model can be fine-tuned on specific tasks or domains to improve its performance.\\n\\n4. **Generative Capability**: Large language models like GPT-3 can generate human-like text based on the input provided to them. They do this by predicting the next word or token in a sequence of text, taking into account the context of the input. This generative capability allows them to write articles, answer questions, or even engage in conversations.\\n\\n5. **Use Cases**: Large language models have a wide range of applications, including natural language understanding, text generation, language translation, chatbots, and more. They are used in various industries for tasks like content creation, customer service, and information retrieval.\\n\\nOverall, large language models like GPT-3 have revolutionized the field of natural language processing and are continuously improving in their ability to understand and generate human language text.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "#String parser to transform aoutput to string\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"input\": \"Can you explain how large language models work?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c5ec11",
   "metadata": {},
   "source": [
    "# Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a25e8e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85895957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "730987c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cb3c6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith can help with testing by allowing you to visualize test results.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "#Chain that takes question and already retrieved document and generate answer\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\": \"how can langsmith help with testing?\",\n",
    "    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e7f2d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith can help with testing by allowing users to debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework. It provides tracing capabilities and evaluation capabilities to assist with testing LLM applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "#Add document retriever to the chain\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f26bbd5",
   "metadata": {},
   "source": [
    "# Conversation Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "951898d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53bc41da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"LangSmith | 🦜️🛠️ LangSmith\\n\\n\\n\\n\\n\\nSkip to main content🦜️🛠️ LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubLangSmithOn this pageLangSmithIntroduction\\u200bLangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the go-to open source framework for building with LLMs.LangSmith is developed by LangChain, the company behind the open source LangChain framework.Quick Start\\u200bTracing: Get started with the tracing quick start.Evaluation: Get started with the evaluation quick start.Next Steps\\u200bCheck out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API key, and configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources\\u200bLangSmith Cookbook: A collection of tutorials and end-to-end walkthroughs using LangSmith.LangChain Python: Docs for the Python LangChain library.LangChain Python API Reference: documentation to review the core APIs of LangChain.LangChain JS: Docs for the TypeScript LangChain libraryDiscord: Join us on our Discord to discuss all things LangChain!Contact SalesIf you're interested in enterprise security and admin features, special deployment options, or access for large teams, reach out to speak with sales.NextUser GuideIntroductionQuick StartNext StepsAdditional ResourcesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright © 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com', 'title': 'LangSmith | 🦜️🛠️ LangSmith', 'description': 'Introduction', 'language': 'en'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "#We make up a chat history\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1aaa0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "663360fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangSmith can help you test your LLM applications by providing tracing and evaluation capabilities. With tracing, you can debug and monitor your chains and intelligent agents built on any LLM framework. The evaluation feature allows you to test and evaluate the performance of your applications. Additionally, LangSmith seamlessly integrates with LangChain, the open source framework for building with LLMs, providing a comprehensive platform for testing and monitoring your LLM applications.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d62bc",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bba4ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.retrievers.tavily_search_api import TavilySearchAPIRetriever\n",
    "\n",
    "#Create retriever tool and search tool\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")\n",
    "\n",
    "search = TavilySearchAPIRetriever(k=3)\n",
    "\n",
    "tools = [retriever_tool, search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433715ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fccc591",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efea0a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"what is the weather in SF?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e577010b",
   "metadata": {},
   "source": [
    "# LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df42589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGENT NOT WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2fe0ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
